{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\deprecation.py:43: UserWarning: You have unused kwarg parameters in Interface, please remove them: {'providers': ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "Running on public URL: https://43854.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://43854.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x2153b24a910>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://43854.gradio.app')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\blocks.py\", line 548, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\blocks.py\", line 463, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\interface.py\", line 514, in <lambda>\n",
      "    lambda *args: self.run_prediction(args)[0]\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\interface.py\", line 718, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"C:\\Users\\Z0047TWV\\AppData\\Local\\Temp\\ipykernel_28348\\813216093.py\", line 9, in inference\n",
      "    result = predict(input_batch)\n",
      "  File \"c:\\Users\\Z0047TWV\\Desktop\\Brain MRI Hackathon\\predict.py\", line 6, in predict\n",
      "    session = onnxruntime.InferenceSession(model_onnx, None)\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py\", line 335, in __init__\n",
      "    self._create_inference_session(providers, provider_options, disabled_optimizers)\n",
      "  File \"c:\\Users\\Z0047TWV\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py\", line 363, in _create_inference_session\n",
      "    raise ValueError(\"This ORT build has {} enabled. \".format(available_providers) +\n",
      "ValueError: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import preprocess\n",
    "from predict import predict\n",
    "\n",
    "def inference(filepath):\n",
    "    input_batch = preprocess(filepath)\n",
    "    result = predict(input_batch)\n",
    "    pred_mask = np.array(result).astype(np.float32)\n",
    "    pred_mask = pred_mask * 255\n",
    "    pred_mask = pred_mask[0, 0, 0, :, :].astype(np.uint8)\n",
    "    plt.imshow(pred_mask)\n",
    "    plt.title(\"Predicted Tumor Mask\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Brain MRI Tumor Segmentation using PyTorch\"\n",
    "description = \"Segmentation of tumor areas from Brain MRI images\"\n",
    "article = \"<p style='text-align: center'><a href='https://www.kaggle.com/s0mnaths/brain-mri-unet-pytorch/' target='_blank'>Kaggle Notebook: Brain MRI-UNET-PyTorch</a> | <a href='https://github.com/s0mnaths/Brain-Tumor-Segmentation' target='_blank'>Github Repo</a></p>\"\n",
    "examples = [['test-samples/TCGA_CS_4942_19970222_10.png'], \n",
    "            ['test-samples/TCGA_CS_4942_19970222_11.png'], \n",
    "            ['test-samples/TCGA_CS_4942_19970222_12.png'], \n",
    "            ['test-samples/TCGA_CS_4941_19960909_15.png']]  \n",
    "\n",
    "gr.Interface(inference, inputs=gr.inputs.Image(type=\"filepath\"), outputs=gr.outputs.Image('plot'), title=title,\n",
    "            description=description, providers= ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'],\n",
    "            article=article,\n",
    "            examples=examples).launch(debug=False, enable_queue=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95cf6bbc3f96a2acf91a41784961e1e94535f1fbd256d5599fd334676c9073e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
